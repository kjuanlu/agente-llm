# Agente LLM local con Ollama + LangChain + FastAPI

## ðŸ§± Requisitos
- Python 3.10+
- Ollama funcionando y modelo descargado
- WSL2 o Linux con entorno virtual activo.

## ðŸš€ Primer uso (una sola vez)
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Opcion A Ejectuar servidor
source .venv/bin/activate
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Opcion B Directamente con docker
docker-compose up --build

# Prueba la API
http://localhost:8000/docs
